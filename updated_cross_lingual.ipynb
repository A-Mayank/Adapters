{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5865006f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Research\\cuda_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Name: NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import AutoModel\n",
    "import torch\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Use only GPU 0\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"0\"  # Enable CUDA DSA for better performance\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "model = AutoModel.from_pretrained(\"xlm-roberta-base\")\n",
    "model = model.to(device)\n",
    "print(\n",
    "    \"GPU Name:\",\n",
    "    torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "341ec44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Research\\cuda_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65068388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sample:\n",
      " {'tokens': ['‡§∏‡•á‡§ï‡•ç‡§ü‡§∞', '55/56', '‡§ï‡•á', '‡§è‡§∏‡§è‡§ö‡§ì', '‡§Ö‡§∞‡§µ‡§ø‡§Ç‡§¶', '‡§ï‡•Å‡§Æ‡§æ‡§∞', '‡§®‡•á', '‡§¨‡§§‡§æ‡§Ø‡§æ', '‡§ï‡§ø', '‡§á‡§∏', '‡§Æ‡§æ‡§Æ‡§≤‡•á', '‡§Æ‡•á‡§Ç', '‡§Ü‡§à‡§™‡•Ä‡§∏‡•Ä', '‡§ï‡•Ä', '‡§ß‡§æ‡§∞‡§æ', '376', '-', '‡§°‡•Ä', '(', '‡§ó‡•à‡§Ç‡§ó‡§∞‡•á‡§™', ')', '‡§ï‡•á', '‡§§‡§π‡§§', '‡§Æ‡§æ‡§Æ‡§≤‡§æ', '‡§¶‡§∞‡•ç‡§ú', '‡§ï‡§∞', '‡§≤‡§ø‡§Ø‡§æ', '‡§ó‡§Ø‡§æ', '‡§π‡•à', '‡•§'], 'ner_tags': [0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "Train size: 927000\n"
     ]
    }
   ],
   "source": [
    "# Define your label mapping\n",
    "label2id = {\n",
    "    \"O\": 0,\n",
    "    \"B-org\": 1,\n",
    "    \"I-org\": 2,\n",
    "    \"B-per\": 3,\n",
    "    \"I-per\": 4,\n",
    "    \"B-geo\": 5,\n",
    "    \"I-geo\": 6,\n",
    "    # Add more if needed\n",
    "}\n",
    "\n",
    "# === Step 1: Load raw dataset ===\n",
    "dataset = load_dataset(\n",
    "    \"text\",\n",
    "    data_files={\n",
    "        \"train\": r\"E:\\Research\\Datasets\\updated_adapter_data\\Source_language( Task adapter)\\hindi\\naamapadam-train_mapped.txt\",\n",
    "        \"test\": r\"E:\\Research\\Datasets\\updated_adapter_data\\Source_language( Task adapter)\\hindi\\naamapadam-test_mapped.txt\",\n",
    "        \"validation\": r\"E:\\Research\\Datasets\\updated_adapter_data\\Source_language( Task adapter)\\hindi\\naamapadam-validation_mapped.txt\",\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "# === Step 2: Parse text lines ===\n",
    "def parse_token_tag_pairs(split):\n",
    "    lines = [line[\"text\"].strip() for line in dataset[split] if line[\"text\"].strip()]\n",
    "    samples = []\n",
    "\n",
    "    for i in range(0, len(lines), 2):\n",
    "        if not lines[i].startswith(\"TOKENS:\") or not lines[i + 1].startswith(\"TAGS:\"):\n",
    "            continue  # Skip invalid\n",
    "\n",
    "        tokens = lines[i].replace(\"TOKENS:\", \"\").strip().split()\n",
    "        tags = lines[i + 1].replace(\"TAGS:\", \"\").strip().split()\n",
    "\n",
    "        if len(tokens) != len(tags):\n",
    "            continue\n",
    "\n",
    "        samples.append(\n",
    "            {\"tokens\": tokens, \"ner_tags\": [label2id.get(tag, 0) for tag in tags]}\n",
    "        )\n",
    "\n",
    "    return samples\n",
    "\n",
    "\n",
    "# === Step 3: Create DatasetDict ===\n",
    "hf_dataset = DatasetDict(\n",
    "    {\n",
    "        \"train\": Dataset.from_list(parse_token_tag_pairs(\"train\")),\n",
    "        \"validation\": Dataset.from_list(parse_token_tag_pairs(\"validation\")),\n",
    "        \"test\": Dataset.from_list(parse_token_tag_pairs(\"test\")),\n",
    "    }\n",
    ")\n",
    "\n",
    "# === Step 4: Inspect ===\n",
    "print(\"Example sample:\\n\", hf_dataset[\"train\"][0])\n",
    "print(\"Train size:\", len(hf_dataset[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1383f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 927000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 13460\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 867\n",
      "    })\n",
      "})\n",
      "Dataset({\n",
      "    features: ['tokens', 'ner_tags'],\n",
      "    num_rows: 50\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(hf_dataset)\n",
    "first_50 = hf_dataset[\"train\"].select(range(50))\n",
    "print(first_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d36fed08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 927000/927000 [06:29<00:00, 2379.43 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13460/13460 [00:05<00:00, 2454.92 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 867/867 [00:00<00:00, 2437.09 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['‡§∏‡•á‡§ï‡•ç‡§ü‡§∞', '55/56', '‡§ï‡•á', '‡§è‡§∏‡§è‡§ö‡§ì', '‡§Ö‡§∞‡§µ‡§ø‡§Ç‡§¶', '‡§ï‡•Å‡§Æ‡§æ‡§∞', '‡§®‡•á', '‡§¨‡§§‡§æ‡§Ø‡§æ', '‡§ï‡§ø', '‡§á‡§∏', '‡§Æ‡§æ‡§Æ‡§≤‡•á', '‡§Æ‡•á‡§Ç', '‡§Ü‡§à‡§™‡•Ä‡§∏‡•Ä', '‡§ï‡•Ä', '‡§ß‡§æ‡§∞‡§æ', '376', '-', '‡§°‡•Ä', '(', '‡§ó‡•à‡§Ç‡§ó‡§∞‡•á‡§™', ')', '‡§ï‡•á', '‡§§‡§π‡§§', '‡§Æ‡§æ‡§Æ‡§≤‡§æ', '‡§¶‡§∞‡•ç‡§ú', '‡§ï‡§∞', '‡§≤‡§ø‡§Ø‡§æ', '‡§ó‡§Ø‡§æ', '‡§π‡•à', '‡•§'], 'ner_tags': [0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [2, 30265, 8430, 4415, 429, 13043, 1883, 90621, 13734, 109832, 22901, 236, 78701, 1134, 1883, 70, 2092, 29073, 2092, 941, 89842, 1883, 14855, 69922, 13, 4086, 20, 2344, 8213, 1551, 28, 1883, 1694, 2092, 29073, 1902, 1325, 68, 55450, 2344, 1134, 4384, 7, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 0, -100, 0, -100, -100, 0, 0, 3, -100, 4, 0, 0, -100, 0, 0, 0, -100, 0, 0, -100, 0, 0, 0, 0, 0, 0, 0, -100, -100, 0, 0, 0, 0, -100, 0, -100, 0, 0, 0, -100, 0, 0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# === Step 4: Tokenization function ===\n",
    "model_checkpoint = \"ai4bharat/indic-bert\"  # üîÅ Change if needed\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)  # üîÅ Change if needed\n",
    "\n",
    "\n",
    "def tokenize_and_align_labels(example):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        example[\"tokens\"],\n",
    "        is_split_into_words=True,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "    previous_word_idx = None\n",
    "\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            labels.append(-100)  # Special tokens\n",
    "        elif word_idx != previous_word_idx:\n",
    "            labels.append(example[\"ner_tags\"][word_idx])  # Use label of first subword\n",
    "        else:\n",
    "            labels.append(-100)  # Ignore subword tokens\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "# Apply the function to all splits\n",
    "tokenized_dataset = hf_dataset.map(tokenize_and_align_labels, batched=False)\n",
    "\n",
    "# Optional check\n",
    "print(tokenized_dataset[\"train\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5942cfb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'ner_tags': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tokenized_dataset[\"train\"].select(range(50))\n",
    "print(train_dataset.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f682e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples after filtering: 927000\n",
      "Validation examples: 13460\n"
     ]
    }
   ],
   "source": [
    "print(\"Training examples after filtering:\", len(tokenized_dataset[\"train\"]))\n",
    "print(\"Validation examples:\", len(tokenized_dataset[\"validation\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b49ad93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'ner_tags': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(tokenized_dataset[\"train\"].features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06ff2e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForTokenClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=len(label2id),  # Ensure this matches your label count\n",
    "    id2label={v: k for k, v in label2id.items()},\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f13316b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 927000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Remove non-numeric columns from tokenized_dataset\n",
    "columns_to_remove = [\"tokens\", \"ner_tags\"]  # keep only model input columns\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(\n",
    "    [col for col in columns_to_remove if col in tokenized_dataset[\"train\"].column_names]\n",
    ")\n",
    "\n",
    "print(\n",
    "    tokenized_dataset[\"train\"]\n",
    ")  # Should only show input_ids, attention_mask, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ccc7bfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from adapters import AutoAdapterModel\n",
    "from transformers import AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"ai4bharat/indic-bert\",\n",
    ")\n",
    "model = AutoAdapterModel.from_pretrained(\n",
    "    \"ai4bharat/indic-bert\",\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c54902",
   "metadata": {},
   "outputs": [],
   "source": [
    "from adapters import AdapterConfig\n",
    "task_adapter_config = AdapterConfig.load(\"pfeiffer\", reduction_factor=16)\n",
    "config = AdapterConfig.load(\"pfeiffer\", non_linearity=\"relu\", reduction_factor=2)\n",
    "model.load_adapter(\"hi/wiki@ukp\", config=config)\n",
    "model.add_adapter(\"hindi_adapter\", config=task_adapter_config)\n",
    "model.add_tagging_head(\"hindi_adapter\", num_labels=len(label2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77a00b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There are adapters available but none are activated for the forward pass.\n"
     ]
    }
   ],
   "source": [
    "model.train_adapter(\"hindi_adapter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13875ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_active_adapters(\"hindi_adapter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0b1a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from adapters.composition import Stack\n",
    "\n",
    "model.active_adapters = Stack(\"hi\", \"hindi_adapter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c422e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\Research\\cuda_env\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from adapters import AdapterTrainer\n",
    "from transformers import TrainingArguments\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=20,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    logging_steps=100,\n",
    "    output_dir=\"./results\",\n",
    "    overwrite_output_dir=True,\n",
    "    # The next line is important to ensure the dataset labels are properly passed to the model\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "train_dataset = concatenate_datasets([dataset[\"train\"], dataset[\"validation\"]])\n",
    "\n",
    "trainer = AdapterTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"].select(range(1000)),\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "770c710e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='640' max='640' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [640/640 04:37, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.929400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.492900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.453500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.420600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.402400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.391400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=640, training_loss=0.5072933301329613, metrics={'train_runtime': 279.6702, 'train_samples_per_second': 71.513, 'train_steps_per_second': 2.288, 'total_flos': 120694563840000.0, 'train_loss': 0.5072933301329613, 'epoch': 20.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2cbce43",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_adapter(\n",
    "    \"./results/hindi_adapter\",\n",
    "    \"hindi_adapter\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a0453444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'TOKENS: ‡§∏‡•á‡§ï‡•ç‡§ü‡§∞ 55/56 ‡§ï‡•á ‡§è‡§∏‡§è‡§ö‡§ì ‡§Ö‡§∞‡§µ‡§ø‡§Ç‡§¶ ‡§ï‡•Å‡§Æ‡§æ‡§∞ ‡§®‡•á ‡§¨‡§§‡§æ‡§Ø‡§æ ‡§ï‡§ø ‡§á‡§∏ ‡§Æ‡§æ‡§Æ‡§≤‡•á ‡§Æ‡•á‡§Ç ‡§Ü‡§à‡§™‡•Ä‡§∏‡•Ä ‡§ï‡•Ä ‡§ß‡§æ‡§∞‡§æ 376 - ‡§°‡•Ä ( ‡§ó‡•à‡§Ç‡§ó‡§∞‡•á‡§™ ) ‡§ï‡•á ‡§§‡§π‡§§ ‡§Æ‡§æ‡§Æ‡§≤‡§æ ‡§¶‡§∞‡•ç‡§ú ‡§ï‡§∞ ‡§≤‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ ‡§π‡•à ‡•§'}\n",
      "{'tokens': ['‡§∏‡•á‡§ï‡•ç‡§ü‡§∞', '55/56', '‡§ï‡•á', '‡§è‡§∏‡§è‡§ö‡§ì', '‡§Ö‡§∞‡§µ‡§ø‡§Ç‡§¶', '‡§ï‡•Å‡§Æ‡§æ‡§∞', '‡§®‡•á', '‡§¨‡§§‡§æ‡§Ø‡§æ', '‡§ï‡§ø', '‡§á‡§∏', '‡§Æ‡§æ‡§Æ‡§≤‡•á', '‡§Æ‡•á‡§Ç', '‡§Ü‡§à‡§™‡•Ä‡§∏‡•Ä', '‡§ï‡•Ä', '‡§ß‡§æ‡§∞‡§æ', '376', '-', '‡§°‡•Ä', '(', '‡§ó‡•à‡§Ç‡§ó‡§∞‡•á‡§™', ')', '‡§ï‡•á', '‡§§‡§π‡§§', '‡§Æ‡§æ‡§Æ‡§≤‡§æ', '‡§¶‡§∞‡•ç‡§ú', '‡§ï‡§∞', '‡§≤‡§ø‡§Ø‡§æ', '‡§ó‡§Ø‡§æ', '‡§π‡•à', '‡•§'], 'ner_tags': [0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][0])\n",
    "print(hf_dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2b6f335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bhojpuri_lang_adapter'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_adapter(\"bhojpuri_lang_adapter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f108d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from adapters.composition import Stack\n",
    "model.active_adapters = Stack(\"bhojpuri_lang_adapter\", \"hindi_adapter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e4441fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sample:\n",
      " {'tokens': ['‡§¶‡•ã‡§∏‡§∞', '‡§à', '‡§ï‡§ø', '‡§à', '‡§™‡•Ç‡§∞‡§æ', '‡§ï‡§π‡§æ‡§®‡•Ä', '‡§ï‡§≤‡•ç‡§™‡§®‡§æ', '‡§π', ',', '‡§ï‡§µ‡§®‡•ã', '‡§ï‡§æ‡§∞‡§£', '‡§∏‡•á', '‡§à', '‡§∏‡§π‡•Ä', '‡§∏‡§æ‡§¨‡§ø‡§§', '‡§π‡•ã', '‡§ú‡§æ‡§µ', '‡§§', '‡§è‡§ï‡§∞', '‡§ú‡§ø‡§Æ‡•ç‡§Æ‡•á‡§¶‡§æ‡§∞‡•Ä', '‡§π‡§Æ‡§∞‡§æ', '‡§™‡§∞', '‡§Æ‡§§', '‡§°‡§æ‡§≤‡§≤', '‡§ú‡§æ‡§µ‡§º', '‡§ï‡§π‡§æ‡§®‡•Ä', '‡•®', '‡§ú‡§µ‡§æ‡§∞', '‡§≠‡§∞', '‡§Æ‡•á‡§Ç', '‡§ï‡•á‡§π‡•Ç', '‡§ï‡•á', '‡§Æ‡§ú‡§æ‡§≤', '‡§®‡§æ', '‡§∞‡§π‡•á', '‡§ï‡§ø', '‡§≠‡•ã‡§≤‡§æ', '‡§™‡§π‡§≤‡§µ‡§æ‡§®', '‡§ï‡§æ', '‡§∏‡•ã‡§ù‡§æ', '‡§ñ‡•ú‡§æ', '‡§π‡•ã‡§ñ‡•á', '.'], 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0]}\n",
      "Train size: 11544\n"
     ]
    }
   ],
   "source": [
    "# Define your label mapping\n",
    "label2id = {\n",
    "    \"O\": 0,\n",
    "    \"B-org\": 1,\n",
    "    \"I-org\": 2,\n",
    "    \"B-per\": 3,\n",
    "    \"I-per\": 4,\n",
    "    \"B-geo\": 5,\n",
    "    \"I-geo\": 6,\n",
    "    # Add more if needed\n",
    "}\n",
    "dataset = load_dataset(\n",
    "    \"text\",\n",
    "    data_files={\n",
    "        \"train\": r\"E:\\Research\\Datasets\\updated_adapter_data\\Target_language( language adapter)\\bhojpuri\\naamapadam-train_mapped.txt\",\n",
    "        \"test\": r\"E:\\Research\\Datasets\\updated_adapter_data\\Target_language( language adapter)\\bhojpuri\\naamapadam-test_mapped.txt\",\n",
    "    },\n",
    ")\n",
    "# === Step 2: Parse text lines ===\n",
    "def parse_token_tag_pairs(split):\n",
    "    lines = [line[\"text\"].strip() for line in dataset[split] if line[\"text\"].strip()]\n",
    "    samples = []\n",
    "\n",
    "    for i in range(0, len(lines), 2):\n",
    "        if not lines[i].startswith(\"TOKENS:\") or not lines[i + 1].startswith(\"TAGS:\"):\n",
    "            continue  # Skip invalid\n",
    "\n",
    "        tokens = lines[i].replace(\"TOKENS:\", \"\").strip().split()\n",
    "        tags = lines[i + 1].replace(\"TAGS:\", \"\").strip().split()\n",
    "\n",
    "        if len(tokens) != len(tags):\n",
    "            continue\n",
    "\n",
    "        samples.append(\n",
    "            {\"tokens\": tokens, \"ner_tags\": [label2id.get(tag, 0) for tag in tags]}\n",
    "        )\n",
    "\n",
    "    return samples\n",
    "\n",
    "\n",
    "# === Step 3: Create DatasetDict ===\n",
    "dataset_bj = DatasetDict(\n",
    "    {\n",
    "        \"train\": Dataset.from_list(parse_token_tag_pairs(\"train\")),\n",
    "        \"test\": Dataset.from_list(parse_token_tag_pairs(\"test\")),\n",
    "    }\n",
    ")\n",
    "\n",
    "# === Step 4: Inspect ===\n",
    "print(\"Example sample:\\n\", dataset_bj[\"train\"][0])\n",
    "print(\"Train size:\", len(dataset_bj[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "77e4f9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'ner_tags': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    dataset_bj[\"train\"].features\n",
    ")  # Should only show input_ids, attention_mask, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ef27d91e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11544/11544 [00:04<00:00, 2764.92 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4948/4948 [00:01<00:00, 2884.95 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_bj = dataset_bj.map(tokenize_and_align_labels, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0af3c0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 11544\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 4948\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset_bj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "de0eafbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForTokenClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=len(label2id),  # Ensure this matches your label count\n",
    "    id2label={v: k for k, v in label2id.items()},\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7b28a973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 927000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Remove non-numeric columns from tokenized_dataset\n",
    "columns_to_remove = [\"tokens\", \"ner_tags\"]  # keep only model input columns\n",
    "dataset_bj = dataset_bj.remove_columns(\n",
    "    [col for col in columns_to_remove if col in dataset_bj[\"train\"].column_names]\n",
    ")\n",
    "\n",
    "print(tokenized_dataset[\"train\"])  # Should only show input_ids, attention_mask, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e9ca6cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 11544\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    dataset_bj[\"train\"]\n",
    ")  # Should only show input_ids, attention_mask, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "844f5192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2062' max='2062' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2062/2062 01:51]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.024326801300049,\n",
       " 'eval_model_preparation_time': 0.0,\n",
       " 'eval_acc': 0.08835633549110958,\n",
       " 'eval_runtime': 111.9441,\n",
       " 'eval_samples_per_second': 147.324,\n",
       " 'eval_steps_per_second': 18.42}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import EvalPrediction\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "def compute_accuracy(p: EvalPrediction):\n",
    "    preds = np.argmax(p.predictions, axis=-1)\n",
    "    mask = p.label_ids != -100\n",
    "    correct = (preds == p.label_ids) & mask\n",
    "    acc = correct.sum() / mask.sum()\n",
    "    return {\"acc\": acc}\n",
    "\n",
    "dataset_bj = concatenate_datasets([dataset_bj[\"train\"], dataset_bj[\"test\"]])\n",
    "eval_trainer = AdapterTrainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./eval_output\",\n",
    "        remove_unused_columns=False,\n",
    "    ),\n",
    "    eval_dataset=dataset_bj,\n",
    "    compute_metrics=compute_accuracy,\n",
    ")\n",
    "eval_trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925fd66e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab46aca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
