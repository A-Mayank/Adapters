{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "392130b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Research\\cuda_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from transformers import AutoModel \n",
    "import torch \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\"\n",
    "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\"\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "model = AutoModel.from_pretrained(\"xlm-roberta-base\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1038f0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 400, 'validation': 100, 'test': 500}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from adapters.composition import Stack\n",
    "\n",
    "dataset_en = load_dataset(\"super_glue\", \"copa\")\n",
    "dataset_en.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46041918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'premise': Value(dtype='string', id=None),\n",
       " 'choice1': Value(dtype='string', id=None),\n",
       " 'choice2': Value(dtype='string', id=None),\n",
       " 'question': Value(dtype='string', id=None),\n",
       " 'idx': Value(dtype='int32', id=None),\n",
       " 'label': ClassLabel(names=['choice1', 'choice2'], id=None)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_en[\"train\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca409283",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "def encode_batch(examples):\n",
    "  \"\"\"Encodes a batch of input data using the model tokenizer.\"\"\"\n",
    "  all_encoded = {\"input_ids\": [], \"attention_mask\": []}\n",
    "  # Iterate through all examples in this batch\n",
    "  for premise, question, choice1, choice2 in zip(examples[\"premise\"], examples[\"question\"], examples[\"choice1\"], examples[\"choice2\"]):\n",
    "    sentences_a = [premise + \" \" + question for _ in range(2)]\n",
    "    # Both answer choices are passed in an array according to the format needed for the multiple-choice prediction head\n",
    "    sentences_b = [choice1, choice2]\n",
    "    encoded = tokenizer(\n",
    "        sentences_a,\n",
    "        sentences_b,\n",
    "        max_length=60,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    all_encoded[\"input_ids\"].append(encoded[\"input_ids\"])\n",
    "    all_encoded[\"attention_mask\"].append(encoded[\"attention_mask\"])\n",
    "  return all_encoded\n",
    "\n",
    "def preprocess_dataset(dataset):\n",
    "  # Encode the input data\n",
    "  dataset = dataset.map(encode_batch, batched=True)\n",
    "  # The transformers model expects the target class column to be named \"labels\"\n",
    "  dataset = dataset.rename_column(\"label\", \"labels\")\n",
    "  # Transform to pytorch tensors and only output the required columns\n",
    "  dataset.set_format(columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "  return dataset\n",
    "\n",
    "dataset_en = preprocess_dataset(dataset_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a17e8cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['premise', 'choice1', 'choice2', 'question', 'idx', 'labels', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 400\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['premise', 'choice1', 'choice2', 'question', 'idx', 'labels', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 100\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['premise', 'choice1', 'choice2', 'question', 'idx', 'labels', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 500\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9f48610",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaAdapterModel were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['heads.default.3.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from adapters import AutoAdapterModel\n",
    "from transformers import AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"xlm-roberta-base\",\n",
    ")\n",
    "model = AutoAdapterModel.from_pretrained(\n",
    "    \"xlm-roberta-base\",\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70c5ee69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The 'config' and 'model_name' arguments are specific to the now unsupported legacy Hub repo and will be removed.Please switch to only providing the HF Model Hub identifier.\n",
      "Automatic redirect to HF Model Hub repo 'AdapterHub/xlm-roberta-base-en-wiki_pfeiffer'. Please switch to the new ID to remove this warning.\n",
      "Fetching 4 files: 100%|██████████| 4/4 [00:00<?, ?it/s]\n",
      "The 'config' and 'model_name' arguments are specific to the now unsupported legacy Hub repo and will be removed.Please switch to only providing the HF Model Hub identifier.\n",
      "Automatic redirect to HF Model Hub repo 'AdapterHub/xlm-roberta-base-zh-wiki_pfeiffer'. Please switch to the new ID to remove this warning.\n",
      "Fetching 4 files: 100%|██████████| 4/4 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from adapters import AdapterConfig\n",
    "\n",
    "# Load the language adapters\n",
    "lang_adapter_config = AdapterConfig.load(\"seq_bn\", reduction_factor=2)\n",
    "model.load_adapter(\"en/wiki@ukp\", config=lang_adapter_config)\n",
    "model.load_adapter(\"zh/wiki@ukp\", config=lang_adapter_config)\n",
    "\n",
    "# Add a new task adapter\n",
    "model.add_adapter(\"copa\")\n",
    "\n",
    "# Add a classification head for our target task\n",
    "model.add_multiple_choice_head(\"copa\", num_choices=2, overwrite_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fed41d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There are adapters available but none are activated for the forward pass.\n"
     ]
    }
   ],
   "source": [
    "model.train_adapter(\"copa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67560db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze and activate stack setup\n",
    "model.active_adapters = Stack(\"en\", \"copa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "162655d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\Research\\cuda_env\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from adapters import AdapterTrainer\n",
    "from transformers import TrainingArguments\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=20,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    logging_steps=100,\n",
    "    output_dir=\"./training_output\",\n",
    "    overwrite_output_dir=True,\n",
    "    # The next line is important to ensure the dataset labels are properly passed to the model\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "train_dataset = concatenate_datasets([dataset_en[\"train\"], dataset_en[\"validation\"]])\n",
    "\n",
    "trainer = AdapterTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c93e45fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 02:40, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.695700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.687500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=320, training_loss=0.6918656349182128, metrics={'train_runtime': 162.1485, 'train_samples_per_second': 61.672, 'train_steps_per_second': 1.974, 'total_flos': 739801173600000.0, 'train_loss': 0.6918656349182128, 'epoch': 20.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3a7ed93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model + adapters saved at ./saved_model_with_adapters\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(\"./saved_model_with_adapters\")\n",
    "tokenizer.save_pretrained(\"./saved_model_with_adapters\")\n",
    "print(\"Full model + adapters saved at ./saved_model_with_adapters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e71e28f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    validation: Dataset({\n",
      "        features: ['premise', 'choice1', 'choice2', 'question', 'label', 'idx', 'changed'],\n",
      "        num_rows: 100\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['premise', 'choice1', 'choice2', 'question', 'label', 'idx', 'changed'],\n",
      "        num_rows: 500\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset_zh = load_dataset(\"xcopa\", \"zh\", verification_mode=\"no_checks\")\n",
    "print(dataset_zh)\n",
    "dataset_zh = preprocess_dataset(dataset_zh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6cbaec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    validation: Dataset({\n",
      "        features: ['premise', 'choice1', 'choice2', 'question', 'labels', 'idx', 'changed', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 100\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['premise', 'choice1', 'choice2', 'question', 'labels', 'idx', 'changed', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 500\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset_zh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef23548a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.active_adapters = Stack(\"zh\", \"copa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eac59f9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.685754656791687,\n",
       " 'eval_model_preparation_time': 0.007,\n",
       " 'eval_acc': 0.586,\n",
       " 'eval_runtime': 5.4054,\n",
       " 'eval_samples_per_second': 92.5,\n",
       " 'eval_steps_per_second': 11.655}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import EvalPrediction\n",
    "\n",
    "\n",
    "def compute_accuracy(p: EvalPrediction):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    mask = p.label_ids != -100\n",
    "    correct = (preds == p.label_ids) & mask\n",
    "    return {\"acc\": (preds == p.label_ids).mean()}\n",
    "\n",
    "\n",
    "eval_trainer = AdapterTrainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./eval_output\",\n",
    "        remove_unused_columns=False,\n",
    "    ),\n",
    "    eval_dataset=dataset_zh[\"test\"],\n",
    "    compute_metrics=compute_accuracy,\n",
    ")\n",
    "eval_trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "90257533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 她想出去。\n"
     ]
    }
   ],
   "source": [
    "premise = \"她拿着钥匙开门。\"\n",
    "question = \"为什么？\"\n",
    "choices = [\"她想进屋。\", \"她想出去。\"]\n",
    "\n",
    "# Build multiple-choice input\n",
    "sentences_a = [premise + \" \" + question for _ in choices]\n",
    "sentences_b = choices\n",
    "# Tokenize\n",
    "encoded = tokenizer(\n",
    "    sentences_a, sentences_b, return_tensors=\"pt\", padding=True, truncation=True\n",
    ")\n",
    "# Move to correct device\n",
    "encoded = {k: v.to(model.device) for k, v in encoded.items()}\n",
    "# Run model\n",
    "outputs = model(**encoded)\n",
    "logits = outputs.logits  # Shape: [batch_size=1, num_choices=2]\n",
    "# Get prediction\n",
    "pred = logits.argmax(dim=1).item()\n",
    "print(f\"Prediction: {choices[pred]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
