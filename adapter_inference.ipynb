{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3c9a7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qq adapters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca8cfdfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Research\\cuda_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of BertAdapterModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['heads.default.3.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from adapters import AutoAdapterModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoAdapterModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e2dc90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The 'config' and 'model_name' arguments are specific to the now unsupported legacy Hub repo and will be removed.Please switch to only providing the HF Model Hub identifier.\n",
      "Automatic redirect to HF Model Hub repo 'AdapterHub/bert-base-uncased_qa_squad1_houlsby'. Please switch to the new ID to remove this warning.\n",
      "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]e:\\Research\\cuda_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\DELL\\.cache\\huggingface\\hub\\models--AdapterHub--bert-base-uncased_qa_squad1_houlsby. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:02<00:00,  2.48it/s]\n"
     ]
    }
   ],
   "source": [
    "qa_adapter = model.load_adapter(\"squad1\", source=\"hf\", config=\"houlsby\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d81d0ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There are adapters available but none are activated for the forward pass.\n"
     ]
    }
   ],
   "source": [
    "model.set_active_adapters(qa_adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d348d5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The model 'BertAdapterModel' is not supported for . Supported models are ['AlbertForQuestionAnswering', 'BartForQuestionAnswering', 'BertForQuestionAnswering', 'BigBirdForQuestionAnswering', 'BigBirdPegasusForQuestionAnswering', 'BloomForQuestionAnswering', 'CamembertForQuestionAnswering', 'CanineForQuestionAnswering', 'ConvBertForQuestionAnswering', 'Data2VecTextForQuestionAnswering', 'DebertaForQuestionAnswering', 'DebertaV2ForQuestionAnswering', 'DiffLlamaForQuestionAnswering', 'DistilBertForQuestionAnswering', 'ElectraForQuestionAnswering', 'ErnieForQuestionAnswering', 'ErnieMForQuestionAnswering', 'FalconForQuestionAnswering', 'FlaubertForQuestionAnsweringSimple', 'FNetForQuestionAnswering', 'FunnelForQuestionAnswering', 'GPT2ForQuestionAnswering', 'GPTNeoForQuestionAnswering', 'GPTNeoXForQuestionAnswering', 'GPTJForQuestionAnswering', 'IBertForQuestionAnswering', 'LayoutLMv2ForQuestionAnswering', 'LayoutLMv3ForQuestionAnswering', 'LEDForQuestionAnswering', 'LiltForQuestionAnswering', 'LlamaForQuestionAnswering', 'LongformerForQuestionAnswering', 'LukeForQuestionAnswering', 'LxmertForQuestionAnswering', 'MarkupLMForQuestionAnswering', 'MBartForQuestionAnswering', 'MegaForQuestionAnswering', 'MegatronBertForQuestionAnswering', 'MistralForQuestionAnswering', 'MixtralForQuestionAnswering', 'MobileBertForQuestionAnswering', 'ModernBertForQuestionAnswering', 'MPNetForQuestionAnswering', 'MptForQuestionAnswering', 'MraForQuestionAnswering', 'MT5ForQuestionAnswering', 'MvpForQuestionAnswering', 'NemotronForQuestionAnswering', 'NezhaForQuestionAnswering', 'NystromformerForQuestionAnswering', 'OPTForQuestionAnswering', 'QDQBertForQuestionAnswering', 'Qwen2ForQuestionAnswering', 'Qwen2MoeForQuestionAnswering', 'Qwen3ForQuestionAnswering', 'Qwen3MoeForQuestionAnswering', 'ReformerForQuestionAnswering', 'RemBertForQuestionAnswering', 'RobertaForQuestionAnswering', 'RobertaPreLayerNormForQuestionAnswering', 'RoCBertForQuestionAnswering', 'RoFormerForQuestionAnswering', 'SplinterForQuestionAnswering', 'SqueezeBertForQuestionAnswering', 'T5ForQuestionAnswering', 'UMT5ForQuestionAnswering', 'XLMForQuestionAnsweringSimple', 'XLMRobertaForQuestionAnswering', 'XLMRobertaXLForQuestionAnswering', 'XLNetForQuestionAnsweringSimple', 'XmodForQuestionAnswering', 'YosoForQuestionAnswering'].\n"
     ]
    }
   ],
   "source": [
    "from transformers import QuestionAnsweringPipeline\n",
    "\n",
    "# Although the Hugging Face QuestionAnsweringPipeline does not officially support the BertAdapterModel, it does work when a qa header is enabled.\n",
    "qa = QuestionAnsweringPipeline(model=model, tokenizer=tokenizer)\n",
    "\n",
    "context = \"\"\"\n",
    "The current modus operandi in NLP involves downloading and fine-tuning pre-trained models consisting of millions or billions of parameters.\n",
    "Storing and sharing such large trained models is expensive, slow, and time-consuming, which impedes progress towards more general and versatile NLP methods that learn from and for many tasks.\n",
    "Adapters -- small learnt bottleneck layers inserted within each layer of a pre-trained model -- ameliorate this issue by avoiding full fine-tuning of the entire model.\n",
    "However, sharing and integrating adapter layers is not straightforward.\n",
    "We propose AdapterHub, a framework that allows dynamic \"stitching-in\" of pre-trained adapters for different tasks and languages.\n",
    "The framework, built on top of the popular HuggingFace Transformers library, enables extremely easy and quick adaptations of state-of-the-art pre-trained models (e.g., BERT, RoBERTa, XLM-R) across tasks and languages.\n",
    "Downloading, sharing, and training adapters is as seamless as possible using minimal changes to the training scripts and a specialized infrastructure.\n",
    "Our framework enables scalable and easy access to sharing of task-specific models, particularly in low-resource scenarios.\n",
    "AdapterHub includes all recent adapter architectures and can be found at AdapterHub.ml.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0de59c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore all FutureWarnings\n",
    "from warnings import simplefilter\n",
    "\n",
    "simplefilter(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0ab211d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❔ What are Adapters?\n",
      "💡 small learnt bottleneck layers inserted within each layer of a pre-trained model\n",
      "\n",
      "❔ What do Adapters avoid?\n",
      "💡 full fine-tuning of the entire model\n",
      "\n",
      "❔ What is proposed?\n",
      "💡 AdapterHub\n",
      "\n",
      "❔ What does AdapterHub allow?\n",
      "💡 dynamic \"stitching-in\"\n",
      "\n",
      "❔ Where can I find AdapterHub?\n",
      "💡 AdapterHub.ml\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def answer_questions(questions):\n",
    "    for question in questions:\n",
    "        result = qa(question=question, context=context)\n",
    "        print(\"❔\", question)\n",
    "        print(\"💡\", result[\"answer\"])\n",
    "        print()\n",
    "\n",
    "\n",
    "answer_questions(\n",
    "    [\n",
    "        \"What are Adapters?\",\n",
    "        \"What do Adapters avoid?\",\n",
    "        \"What is proposed?\",\n",
    "        \"What does AdapterHub allow?\",\n",
    "        \"Where can I find AdapterHub?\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f298140",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a767774b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ac6e5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4bfdf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
