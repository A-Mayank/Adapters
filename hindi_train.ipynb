{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d45690",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoModel\n",
    "import torch\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Use only GPU 0\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"0\"  # Enable CUDA DSA for better performance\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "model = AutoModel.from_pretrained(\"xlm-roberta-base\")\n",
    "model = model.to(device)\n",
    "print(\n",
    "    \"GPU Name:\",\n",
    "    torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4686b052",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "\n",
    "# Define your NER label mapping\n",
    "label2id = {\n",
    "    \"O\": 0,  # Outside any named entity\n",
    "    \"B-per\": 1,\n",
    "    \"I-per\": 2,\n",
    "    \"B-org\": 3,\n",
    "    \"I-org\": 4,\n",
    "    \"B-loc\": 5,\n",
    "    \"I-loc\": 6,\n",
    "}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "num_labels = len(label2id)\n",
    "\n",
    "\n",
    "# Generic parsing function for a dataset file\n",
    "def parse_token_tag_file(filepath, label2id):\n",
    "    with open(filepath, encoding=\"utf-8\") as f:\n",
    "        lines = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "    samples = []\n",
    "    for i in range(0, len(lines), 2):\n",
    "        if not lines[i].startswith(\"TOKENS:\") or not lines[i + 1].startswith(\"TAGS:\"):\n",
    "            continue\n",
    "\n",
    "        tokens = lines[i].replace(\"TOKENS:\", \"\").strip().split()\n",
    "        tags = lines[i + 1].replace(\"TAGS:\", \"\").strip().split()\n",
    "\n",
    "        if len(tokens) != len(tags):\n",
    "            continue\n",
    "\n",
    "        samples.append(\n",
    "            {\"tokens\": tokens, \"ner_tags\": [label2id.get(tag, 0) for tag in tags]}\n",
    "        )\n",
    "\n",
    "    return samples\n",
    "\n",
    "\n",
    "# Load Hindi dataset\n",
    "hindi_train = parse_token_tag_file(\n",
    "    \"Datasets/updated_adapter_data/Source_language( Task adapter)/hindi/naamapadam-train_mapped.txt\",\n",
    "    label2id,\n",
    ")\n",
    "hindi_val = parse_token_tag_file(\n",
    "    \"Datasets/updated_adapter_data/Source_language( Task adapter)/hindi/naamapadam-test_mapped.txt\",\n",
    "    label2id,\n",
    ")\n",
    "\n",
    "hindi_dataset = DatasetDict(\n",
    "    {\n",
    "        \"train\": Dataset.from_list(hindi_train),\n",
    "        \"validation\": Dataset.from_list(hindi_val),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Load Bhojpuri dataset\n",
    "bhojpuri_train = parse_token_tag_file(\n",
    "    \"Datasets/bhojpuri/naamapadam-train_mapped.txt\", label2id\n",
    ")\n",
    "bhojpuri_test = parse_token_tag_file(\n",
    "    \"Datasets/bhojpuri/naamapadam-test_mapped.txt\", label2id\n",
    ")\n",
    "\n",
    "bhojpuri_dataset = DatasetDict(\n",
    "    {\n",
    "        \"train\": Dataset.from_list(bhojpuri_train),\n",
    "        \"test\": Dataset.from_list(bhojpuri_test),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b32ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "model_name = \"xlm-roberta-base\"  # or \"ai4bharat/indic-bert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(label2id),\n",
    "    label2id=label2id,\n",
    "    id2label={v: k for k, v in label2id.items()},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c6842d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(example):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        example[\"tokens\"],\n",
    "        is_split_into_words=True,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "    previous_word_idx = None\n",
    "\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            labels.append(-100)\n",
    "        elif word_idx != previous_word_idx:\n",
    "            labels.append(example[\"ner_tags\"][word_idx])\n",
    "        else:\n",
    "            labels.append(-100)\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3210ee5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi_tokenized = hindi_dataset.map(tokenize_and_align_labels, batched=False)\n",
    "bhojpuri_tokenized = bhojpuri_dataset.map(tokenize_and_align_labels, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a0bfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-numeric columns from tokenized_dataset\n",
    "columns_to_remove = [\"tokens\", \"ner_tags\"]  # keep only model input columns\n",
    "hi_dataset = hindi_tokenized.remove_columns(\n",
    "    [col for col in columns_to_remove if col in hindi_tokenized[\"train\"].column_names]\n",
    ")\n",
    "bj_dataset = bhojpuri_tokenized.remove_columns(\n",
    "    [\n",
    "        col\n",
    "        for col in columns_to_remove\n",
    "        if col in bhojpuri_tokenized[\"train\"].column_names\n",
    "    ]\n",
    ")\n",
    "print(\n",
    "    hi_dataset[\"train\"],\n",
    "    bj_dataset[\"train\"],  # Should only show input_ids, attention_mask, labels\n",
    ")  # Should only show input_ids, attention_mask, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeed1dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./hindi_ner_model\",\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=20,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    remove_unused_columns=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e6d4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=hi_dataset[\"train\"],  # Use a subset for faster training\n",
    "    eval_dataset=bj_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32737de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf83e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global store for confusion matrix\n",
    "all_true_labels = []\n",
    "all_pred_labels = []\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    import numpy as np\n",
    "\n",
    "    global all_true_labels, all_pred_labels\n",
    "\n",
    "    preds = np.argmax(p.predictions, axis=-1)\n",
    "    labels = p.label_ids\n",
    "\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "\n",
    "    for pred_seq, label_seq in zip(preds, labels):\n",
    "        for pred_idx, label_idx in zip(pred_seq, label_seq):\n",
    "            if label_idx == -100:\n",
    "                continue  # Skip padding\n",
    "            true_tag = id2label[int(label_idx)]\n",
    "            pred_tag = id2label[int(pred_idx)]\n",
    "            if true_tag != \"O\":  # Ignore 'O' tags\n",
    "                true_labels.append(true_tag)\n",
    "                pred_labels.append(pred_tag)\n",
    "\n",
    "    # Store for confusion matrix\n",
    "    all_true_labels = true_labels\n",
    "    all_pred_labels = pred_labels\n",
    "\n",
    "    # Compute metrics\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        true_labels, pred_labels, average=\"macro\", zero_division=0\n",
    "    )\n",
    "    acc = accuracy_score(true_labels, pred_labels)\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"accuracy\": round(acc, 4),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbde96ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "eval_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./eval_output\",\n",
    "        remove_unused_columns=False,\n",
    "    ),\n",
    "    eval_dataset=bj_dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "eval_trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00743007",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(true_labels, pred_labels, labels):\n",
    "    cm = confusion_matrix(true_labels, pred_labels, labels=labels)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels\n",
    "    )\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(\"NER Confusion Matrix (excluding 'O')\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Call this after evaluation\n",
    "unique_labels = sorted(set(all_true_labels + all_pred_labels))\n",
    "plot_confusion_matrix(all_true_labels, all_pred_labels, labels=unique_labels)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
