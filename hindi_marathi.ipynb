{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ab1cd1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.5.1+cu121\n",
      "CUDA Available: True\n",
      "GPU Name: NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\n",
    "    \"GPU Name:\",\n",
    "    torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f17b9285",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from datasets import Dataset, DatasetDict, load_dataset, concatenate_datasets\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34cd27df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sample:\n",
      " {'tokens': ['‡§∏‡•á‡§ï‡•ç‡§ü‡§∞', '55/56', '‡§ï‡•á', '‡§è‡§∏‡§è‡§ö‡§ì', '‡§Ö‡§∞‡§µ‡§ø‡§Ç‡§¶', '‡§ï‡•Å‡§Æ‡§æ‡§∞', '‡§®‡•á', '‡§¨‡§§‡§æ‡§Ø‡§æ', '‡§ï‡§ø', '‡§á‡§∏', '‡§Æ‡§æ‡§Æ‡§≤‡•á', '‡§Æ‡•á‡§Ç', '‡§Ü‡§à‡§™‡•Ä‡§∏‡•Ä', '‡§ï‡•Ä', '‡§ß‡§æ‡§∞‡§æ', '376', '-', '‡§°‡•Ä', '(', '‡§ó‡•à‡§Ç‡§ó‡§∞‡•á‡§™', ')', '‡§ï‡•á', '‡§§‡§π‡§§', '‡§Æ‡§æ‡§Æ‡§≤‡§æ', '‡§¶‡§∞‡•ç‡§ú', '‡§ï‡§∞', '‡§≤‡§ø‡§Ø‡§æ', '‡§ó‡§Ø‡§æ', '‡§π‡•à', '‡•§'], 'ner_tags': [0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "Train size: 1382248\n"
     ]
    }
   ],
   "source": [
    "# Define your label mapping\n",
    "label2id = {\n",
    "    \"O\": 0,\n",
    "    \"B-org\": 1,\n",
    "    \"I-org\": 2,\n",
    "    \"B-per\": 3,\n",
    "    \"I-per\": 4,\n",
    "    \"B-geo\": 5,\n",
    "    \"I-geo\": 6,\n",
    "    # Add more if needed\n",
    "}\n",
    "\n",
    "# === Step 1: Load raw dataset ===\n",
    "hindi_dataset = load_dataset(\n",
    "    \"text\",\n",
    "    data_files={\n",
    "        \"train\": r\"E:\\Research\\Datasets\\updated_adapter_data\\Source_language( Task adapter)\\hindi\\naamapadam-train_mapped.txt\",\n",
    "        \"test\": r\"E:\\Research\\Datasets\\updated_adapter_data\\Source_language( Task adapter)\\hindi\\naamapadam-test_mapped.txt\",\n",
    "        \"validation\": r\"E:\\Research\\Datasets\\updated_adapter_data\\Source_language( Task adapter)\\hindi\\naamapadam-validation_mapped.txt\",\n",
    "    },\n",
    ")\n",
    "marathi_dataset = load_dataset(\n",
    "    \"text\",\n",
    "    data_files={\n",
    "        \"train\": r\"E:\\Research\\Datasets\\Adapter_NER_data\\Source_language( Task adapter)\\marathi\\naamapadam-train_mapped.txt\",\n",
    "        \"test\": r\"E:\\Research\\Datasets\\Adapter_NER_data\\Source_language( Task adapter)\\marathi\\naamapadam-test_mapped.txt\",\n",
    "        \"validation\": r\"E:\\Research\\Datasets\\Adapter_NER_data\\Source_language( Task adapter)\\marathi\\naamapadam-validation_mapped.txt\",\n",
    "    },\n",
    ")\n",
    "dataset = {\n",
    "    \"train\": concatenate_datasets([hindi_dataset[\"train\"], marathi_dataset[\"train\"]]),\n",
    "    \"validation\": concatenate_datasets([hindi_dataset[\"validation\"], marathi_dataset[\"validation\"]])\n",
    "}\n",
    "\n",
    "# === Step 2: Parse text lines ===\n",
    "def parse_token_tag_pairs(split):\n",
    "    lines = [line[\"text\"].strip() for line in dataset[split] if line[\"text\"].strip()]\n",
    "    samples = []\n",
    "\n",
    "    for i in range(0, len(lines), 2):\n",
    "        if not lines[i].startswith(\"TOKENS:\") or not lines[i + 1].startswith(\"TAGS:\"):\n",
    "            continue  # Skip invalid\n",
    "\n",
    "        tokens = lines[i].replace(\"TOKENS:\", \"\").strip().split()\n",
    "        tags = lines[i + 1].replace(\"TAGS:\", \"\").strip().split()\n",
    "\n",
    "        if len(tokens) != len(tags):\n",
    "            continue\n",
    "\n",
    "        samples.append(\n",
    "            {\"tokens\": tokens, \"ner_tags\": [label2id.get(tag, 0) for tag in tags]}\n",
    "        )\n",
    "\n",
    "    return samples\n",
    "\n",
    "\n",
    "# === Step 3: Create DatasetDict ===\n",
    "hf_dataset = DatasetDict(\n",
    "    {\n",
    "        \"train\": Dataset.from_list(parse_token_tag_pairs(\"train\")),\n",
    "        \"validation\": Dataset.from_list(parse_token_tag_pairs(\"validation\")),\n",
    "    }\n",
    ")\n",
    "\n",
    "# === Step 4: Inspect ===\n",
    "print(\"Example sample:\\n\", hf_dataset[\"train\"][0])\n",
    "print(\"Train size:\", len(hf_dataset[\"train\"]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0aaa3614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 1382248\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 15760\n",
      "    })\n",
      "})\n",
      "Dataset({\n",
      "    features: ['tokens', 'ner_tags'],\n",
      "    num_rows: 50\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(hf_dataset)\n",
    "first_50 = hf_dataset[\"train\"].select(range(50))\n",
    "print(first_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d8c5ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1382248/1382248 [15:36<00:00, 1476.13 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15760/15760 [00:06<00:00, 2301.75 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['‡§∏‡•á‡§ï‡•ç‡§ü‡§∞', '55/56', '‡§ï‡•á', '‡§è‡§∏‡§è‡§ö‡§ì', '‡§Ö‡§∞‡§µ‡§ø‡§Ç‡§¶', '‡§ï‡•Å‡§Æ‡§æ‡§∞', '‡§®‡•á', '‡§¨‡§§‡§æ‡§Ø‡§æ', '‡§ï‡§ø', '‡§á‡§∏', '‡§Æ‡§æ‡§Æ‡§≤‡•á', '‡§Æ‡•á‡§Ç', '‡§Ü‡§à‡§™‡•Ä‡§∏‡•Ä', '‡§ï‡•Ä', '‡§ß‡§æ‡§∞‡§æ', '376', '-', '‡§°‡•Ä', '(', '‡§ó‡•à‡§Ç‡§ó‡§∞‡•á‡§™', ')', '‡§ï‡•á', '‡§§‡§π‡§§', '‡§Æ‡§æ‡§Æ‡§≤‡§æ', '‡§¶‡§∞‡•ç‡§ú', '‡§ï‡§∞', '‡§≤‡§ø‡§Ø‡§æ', '‡§ó‡§Ø‡§æ', '‡§π‡•à', '‡•§'], 'ner_tags': [0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [2, 30265, 8430, 4415, 429, 13043, 1883, 90621, 13734, 109832, 22901, 236, 78701, 1134, 1883, 70, 2092, 29073, 2092, 941, 89842, 1883, 14855, 69922, 13, 4086, 20, 2344, 8213, 1551, 28, 1883, 1694, 2092, 29073, 1902, 1325, 68, 55450, 2344, 1134, 4384, 7, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 0, -100, 0, -100, -100, 0, 0, 3, -100, 4, 0, 0, -100, 0, 0, 0, -100, 0, 0, -100, 0, 0, 0, 0, 0, 0, 0, -100, -100, 0, 0, 0, 0, -100, 0, -100, 0, 0, 0, -100, 0, 0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}\n"
     ]
    }
   ],
   "source": [
    "# === Step 4: Tokenization function ===\n",
    "model_checkpoint = \"ai4bharat/indic-bert\"  # üîÅ Change if needed\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)  # üîÅ Change if needed\n",
    "\n",
    "\n",
    "def tokenize_and_align_labels(example):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        example[\"tokens\"],\n",
    "        is_split_into_words=True,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "    previous_word_idx = None\n",
    "\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            labels.append(-100)  # Special tokens\n",
    "        elif word_idx != previous_word_idx:\n",
    "            labels.append(example[\"ner_tags\"][word_idx])  # Use label of first subword\n",
    "        else:\n",
    "            labels.append(-100)  # Ignore subword tokens\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "# Apply the function to all splits\n",
    "tokenized_dataset = hf_dataset.map(tokenize_and_align_labels, batched=False)\n",
    "\n",
    "# Optional check\n",
    "print(tokenized_dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16a63ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'ner_tags': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tokenized_dataset[\"train\"].select(range(50))\n",
    "print(train_dataset.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5f751f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples after filtering: 1382248\n",
      "Validation examples: 15760\n"
     ]
    }
   ],
   "source": [
    "print(\"Training examples after filtering:\", len(tokenized_dataset[\"train\"]))\n",
    "print(\"Validation examples:\", len(tokenized_dataset[\"validation\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d7b0be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'ner_tags': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset[\"train\"].features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5689fdb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForTokenClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=len(label2id),  # Ensure this matches your label count\n",
    "    id2label={v: k for k, v in label2id.items()},\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e74c62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 1382248\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Remove non-numeric columns from tokenized_dataset\n",
    "columns_to_remove = [\"tokens\", \"ner_tags\"]  # keep only model input columns\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(\n",
    "    [col for col in columns_to_remove if col in tokenized_dataset[\"train\"].column_names]\n",
    ")\n",
    "\n",
    "print(tokenized_dataset[\"train\"])  # Should only show input_ids, attention_mask, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89758ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from adapters import AutoAdapterModel\n",
    "from transformers import AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"ai4bharat/indic-bert\",\n",
    ")\n",
    "model = AutoAdapterModel.from_pretrained(\n",
    "    \"ai4bharat/indic-bert\",\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffc81254",
   "metadata": {},
   "outputs": [],
   "source": [
    "from adapters import AdapterConfig\n",
    "\n",
    "task_adapter_config = AdapterConfig.load(\"pfeiffer\", reduction_factor=16)\n",
    "\"\"\"config = AdapterConfig.load(\"pfeiffer\", non_linearity=\"relu\", reduction_factor=2)\n",
    "model.load_adapter(\"hi/wiki@ukp\", config=config)\"\"\"\n",
    "model.add_adapter(\"hindi_marathi_adapter\", config=task_adapter_config)\n",
    "model.add_tagging_head(\"hindi_marathi_adapter\", num_labels=len(label2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54e460b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There are adapters available but none are activated for the forward pass.\n"
     ]
    }
   ],
   "source": [
    "model.train_adapter(\"hindi_marathi_adapter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb53df85",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_active_adapters(\"hindi_marathi_adapter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8500c9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from adapters import AdapterTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=20,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    logging_steps=100,\n",
    "    output_dir=\"./hindi_marathi\",\n",
    "    overwrite_output_dir=True,\n",
    "    # The next line is important to ensure the dataset labels are properly passed to the model\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = AdapterTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc33311",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a66e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_adapter(\n",
    "    \"./results/hindi_marathi\",\n",
    "    \"hindi_marathi_adapter\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
